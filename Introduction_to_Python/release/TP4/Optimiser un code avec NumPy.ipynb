{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27260743",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.ensam.eu/logo/fr/logo-trans-322x84.png\" width=\"256px\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8626d3",
   "metadata": {},
   "source": [
    "Avant de me rendre votre notebook, assurez-vous que tout tourne comme attendu.\n",
    "Pour cela, **redémarrer le kernel** (dans la barre de menu, selectionnez *Kernel $\\rightarrow$ Restart*) et ensuite **exécutez toutes les cellules** (dans la barre de menu, selectionnez *Cellules $\\rightarrow$ Exécuter toutes les cellules*).\n",
    "\n",
    "Assurez-vous par ailleurs d'avoir écrit votre code partout où il est dit `YOUR CODE HERE` ainsi que les réponses libres où il est écrit \"YOUR ANSWER HERE\".\n",
    "Enfin, rentrez votre nom et celui des personnes avec qui vous avez travaillés ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604422b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c03b43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5697ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95f51bbb47df7c4becf0f719f0ba570c",
     "grade": false,
     "grade_id": "cell-0f9756f9f3831234",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd291b2",
   "metadata": {},
   "source": [
    "# Bonnes pratiques en NumPy\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/NumPy_logo_2020.svg/1280px-NumPy_logo_2020.svg.png\" width=\"256px\" align=\"left\"/>\n",
    "\n",
    "Les séances précédentes avaient pour objectifs de vous familiariser avec les différentes librairies formant l'écosystème `python scientifique`.\n",
    "Nous nous sommes notamment intéresser à la bibliothèque **NumPy**.\n",
    "Cette dernière fournie toutes les briques de base nécessaires pour le calcul scientifique en `python`, notamment la structure données connue sous le nom de `numpy array`.\n",
    "**NumPy** fournit également une ensemble de fonctions mathématiques opérant sur ces tableaux.\n",
    "Bien qu'il s'agisse d'une bibliothèque Python, la plupart de ses fonctions sont écrites dans un langage de plus bas niveau tel que [Fortran](https://fr.m.wikipedia.org/wiki/Fortran) ou [C](https://fr.m.wikipedia.org/wiki/C_(langage)).\n",
    "L'utilisation de tels langages pour la partie calculatoire permet de garantir d'excellentes performances tandis que l'interface Python simplifie fortement l'utilisation de ces fonctions.\n",
    "\n",
    "Dans le cadre du présent TP, nous allons maintenant nous intéresser aux bonnes pratiques devant être mises en oeuvre afin de tirer parti au maximum des performances de **NumPy**.\n",
    "Pour cela, plusieurs bouts de code vous sont présentés ci-dessous.\n",
    "Pour chacun d'entre eux, votre objectif est non seulement de comprendre ce qu'ils font (une rapide description vous est néanmoins donnée en préambule), mais aussi de les ré-écrire de façon plus efficace (notamment en *vectorisant* ce qui peut être vectorisé)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c58b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Méthode des moindres carrés\n",
    "\n",
    "Le cadre d'application de ce TP sera la [méthode des moindres carrés](https://fr.wikipedia.org/wiki/M%C3%A9thode_des_moindres_carr%C3%A9s).\n",
    "Cette méthode, développée indépendamment au début du XIXe siècle par le mathématicien français [Adrien-Marie Legendre](https://fr.wikipedia.org/wiki/Adrien-Marie_Legendre) et le mathématicien Allemand [Carl Friedrich Gauss](https://fr.wikipedia.org/wiki/Carl_Friedrich_Gauss) permet de comparer des données expérimentales (généralement entachées d'erreurs de mesure) à un modèle mathématique censé les décrire.\n",
    "Elle repose sur la formulation d'un problème d'optimisation permettant de trouver la meilleure estimation possible des paramètres de ce modèle.\n",
    "\n",
    "Afin de limiter la difficulté de ce TP, nous nous limiterons à la méthode dite des *moindres carrés linéaire*.\n",
    "Etant donné un ensemble de données $(x_i, y_i)$ (où $x_i$ est l'abscisse des points de données et $y_i$ leur ordonnée), on supposera alors que ces données peuvent être décrites par le modèle suivant\n",
    "\n",
    "$$\n",
    "y_i \\simeq a x_i + b\n",
    "$$\n",
    "\n",
    "où $a$ et $b$ sont les paramètres que l'on cherche à déterminer.\n",
    "La figure ci-dessous illustre le jeu de données avec lequel nous travaillerons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26523e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "651bd2057efc1d1444f4b95d9790c0fe",
     "grade": false,
     "grade_id": "cell-c591a6e111d41529",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(n=512, σ=0.5, a=1.0, b=0.5):\n",
    "    \"\"\"\n",
    "    Fonction permettant de générer un jeu de données aléatoire.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --> Génère aléatoirement les abscisses des points.\n",
    "    x = npr.normal(loc=0, scale=3, size=n)\n",
    "    \n",
    "    # --> Génère la droite déterministe.\n",
    "    y = a*x + b\n",
    "    \n",
    "    # --> Ajoute du bruit.\n",
    "    y += npr.normal(loc=0, scale=σ, size=n)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# --> Génération des données dont vous aurez besoin.\n",
    "x, y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b7042",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9b96bdb32e780c802823e88f6432299",
     "grade": false,
     "grade_id": "cell-601da5c8382dec25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.2, label=\"Données\")\n",
    "\n",
    "ax.set(xlim=(-10, 10), ylim=(-10, 10))\n",
    "ax.set(xlabel=\"x\", ylabel=\"y\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfcdd7",
   "metadata": {},
   "source": [
    "En regardant cette figure, une relation linéaire entre $x_i$ et $y_i$ est assez évidente et notre modèle mathématique semble donc plausible.\n",
    "La question à laquelle nous souhaiterons répondre est donc : comment calculer la meilleure estimation possible des paramètres $a$ et $b$ de notre modèle ?\n",
    "\n",
    "### Formulation mathématique\n",
    "\n",
    "Afin de répondre à cette question, il nous est d'abord nécessaire de définir une notion de l'erreur entre les prédictions $\\hat{y}_i$ de notre modèle et nos observations expérimentales $y_i$.\n",
    "Plusieurs définitions ont été proposées pendant la séance de cours.\n",
    "La définition la plus commune est celle de l'[erreur quadratique moyenne](https://fr.wikipedia.org/wiki/Erreur_quadratique_moyenne).\n",
    "Etant données les prédictions de notre modèle $\\hat{y}_i = a x_i + b$ et les observations $y_i$, cette erreur est définie de la façon suivante\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(a, b) = \\dfrac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "où $n$ est le nombre d'observations disponibles.\n",
    "Cette notion de l'erreur faisant intervenir un carré, elle sera toujours positive.\n",
    "De plus, si notre modèle prédit exactement les observations (i.e. $\\hat{y}_i = y_i \\quad \\forall i$), alors cette erreur vaut 0.\n",
    "Dans le cas contraire, elle sera supérieure à 0.\n",
    "On cherche alors à trouver les valeurs de $a$ et $b$ de façon à minimiser cette erreur au maximum.\n",
    "\n",
    "#### Comment minimiser une fonction ?\n",
    "\n",
    "Lorsqu'une fonction (dérivable au moins une fois) atteint son minimum, le gradient de cette fonction est nul.\n",
    "Minimiser une fonction revient donc à trouver les valeurs des paramètres de façon à ce que le gradient soit nul.\n",
    "Une illustration a été donnée en cours.\n",
    "Il nous est donc nécessaire de trouver une expression analytique du gradient de notre fonction objective.\n",
    "\n",
    "Notre modèle étant donné par\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = a x_i + b,\n",
    "$$\n",
    "\n",
    "il est facile de montrer que la dérivée partielle de notre erreur quadratique moyenne $\\mathcal{L}$ par rapport au coefficient directeur $a$ de la droite que l'on cherche à calculer est donnée par\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial a} = -\\dfrac{2}{n} \\sum_{i=1}^n \\left( y_i - a x_i - b \\right) a_i.\n",
    "$$\n",
    "\n",
    "De la même façon, la dérivée partielle de $\\mathcal{L}$ par rapport à $b$ est donnée par\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial b} = -\\dfrac{2}{n} \\sum_{i=1}^n \\left( y_i - ax_i - b \\right).\n",
    "$$\n",
    "\n",
    "A partir d'une estimation initiale des paramètres $a$ et $b$, le gradient de $\\mathcal{L}$ indique la direction dans laquelle notre erreur augmente.\n",
    "Ainsi, si $\\partial_a \\mathcal{L} > 0$, augmenter légèrement la valeur de $a$ aura tendance à augmenter l'erreur de notre modèle (et similairement pour $b$).\n",
    "Afin de minimiser notre fonction, il sera alors nécessaire de diminuer légèrement la valeur de $a$.\n",
    "De même, si $\\partial_a \\mathcal{L} < 0$, diminuer légèrement la valeur de $a$ aura tendance à augmenter l'erreur de notre modèle.\n",
    "Afin de minimiser notre fonction, il sera alors cette fois ci nécessaire d'augmenter légèrement la valeur de $a$.\n",
    "Les mêmes principes s'appliquent pour $b$.\n",
    "\n",
    "A partir de ce constat, il est alors facile de se convaincre que, étant donnée une estimation initiale des paramètres $a_0$ et $b_0$ du modèle, l'algorithme suivant devrait petit à petit converger vers un minimum de la fonction:\n",
    "\n",
    "1. Calcul de $\\mathcal{L}(a_k, b_k)$ et du gradient $\\nabla \\mathcal{L}$.\n",
    "Si la norme du gradient est suffisament petite, alors on s'arrête car l'on est déjà très proche du minimum.\n",
    "Sinon, on passe à l'étape 2.\n",
    "2. Mise à jour des paramètres par la règle suivante\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{k+1} & = a_k - \\eta \\dfrac{\\partial \\mathcal{L}}{\\partial a} \\\\\n",
    "b_{k+1} & = b_k - \\eta \\dfrac{\\partial \\mathcal{L}}{\\partial b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "où $\\eta$ est le *pas de descente*.\n",
    "En pratique $\\eta$ ne doit pas être trop grand.\n",
    "3. On retourne ensuite à la première étape.\n",
    "\n",
    "Cet algorithme, connu sous le nom de **descente de gradient**, forme aujourd'hui la base de la plupart des méthodes d'optimisation, notamment celles utilisées en Machine Learning et Deep Learning.\n",
    "\n",
    "### Illustration\n",
    "\n",
    "Les cellules ci-dessous illustrent comment il vous est possible d'implémenter un tel algorithme en `python`.\n",
    "Prenez le temps de bien lire les commentaires et de comparer les expressions avec celles données au dessous afin de bien comprendre ce que fait ce code.\n",
    "Votre objectif sera de ré-écrire ce même code mais de façon plus efficace en utilisant au maximum les fonctions déjà présentes dans **NumPy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b98c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b09c213fef0138be5b52272ac410877c",
     "grade": false,
     "grade_id": "cell-79909f728752f19f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def objectif(x, y, a, b):\n",
    "    \"\"\"\n",
    "    Fonction calculant la valeur de la fonction objective\n",
    "    pour les paramètres courants a et b.\n",
    "    \n",
    "    INPUT\n",
    "    -----\n",
    "    \n",
    "    x : Numpy array, shape (n,)\n",
    "        Abscise des points de données.\n",
    "        \n",
    "    y : Numpy array, shape (n,)\n",
    "        Ordonnée des points de données.\n",
    "        \n",
    "    a : float\n",
    "        Cofficient directeur de la droite y = ax + b.\n",
    "        \n",
    "    b : float\n",
    "        Constante de la droite y = ax + b.\n",
    "        \n",
    "    RETURN\n",
    "    ------\n",
    "    \n",
    "    mse : float\n",
    "          Erreur quadratique moyenne calculée à partir\n",
    "          des coefficients a et b du modèle.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --> Nombre de points de données.\n",
    "    n = len(x)\n",
    "    \n",
    "    # --> Calcul de l'erreur quadratique moyenne.\n",
    "    mse = 0.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        mse = mse + (y[i] - (a*x[i] + b))**2\n",
    "        \n",
    "    mse = mse / n\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642db8e",
   "metadata": {},
   "source": [
    "La fonction ci-dessus calcule l'erreur quadratique moyenne de notre modèle (paramètres actuels $a$ et $b$) vis à vis de nos données.\n",
    "La cellule ci-dessous illustre comment tester le temps d'exécution de cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d4e42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9961a65fb4703eaf884743a9daf1fa92",
     "grade": false,
     "grade_id": "cell-8c3dd143d4d6d22c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "objectif(x, y, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db525d7d",
   "metadata": {},
   "source": [
    "Sur mon ordinateur, cette fonction prends de l'ordre de 500 microsecondes pour s'exécuter.\n",
    "Plus tard dans le TP, il vous sera demander de la ré-écrire de façon plus optimisée.\n",
    "En utilisant intelligement la fonction `np.mean` par exemple, il vous sera possible d'augmenter la rapidité d'exécution d'un facteur presque 30 (i.e. 15 microsecondes au lieu de 500). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd6858",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2871c1c8e283091eb71fc3dba1a2b450",
     "grade": false,
     "grade_id": "cell-3a326869aee66405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient(x, y, a, b):\n",
    "    \"\"\"\n",
    "    Fonction calculant le gradient de la fonction objective\n",
    "    (i.e. les dérivées partielles par rapport à a et b).\n",
    "    \n",
    "    INPUT\n",
    "    -----\n",
    "    \n",
    "    x : Numpy array, shape (n,)\n",
    "        Abscise des points de données.\n",
    "        \n",
    "    y : Numpy array, shape (n,)\n",
    "        Ordonnée des points de données.\n",
    "        \n",
    "    a : float\n",
    "        Cofficient directeur de la droite y = ax + b.\n",
    "        \n",
    "    b : float\n",
    "        Constante de la droite y = ax + b.\n",
    "        \n",
    "    RETURN\n",
    "    ------\n",
    "    \n",
    "    da : float\n",
    "         Composante du gradient par rapport à la variable a.\n",
    "         \n",
    "    db : float\n",
    "         Composante du gradient par rapport à la variable b.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --> Nombre de points.\n",
    "    n = len(x)\n",
    "    \n",
    "    # --> Calcul de la composante du gradient associée\n",
    "    #     au paramètre a (coefficient directeur).\n",
    "    da = 0.0\n",
    "    for i in range(n):\n",
    "        da = da - (2/n) * (y[i] - a*x[i] - b) * x[i]\n",
    "\n",
    "    # --> Calcul de la composante du gradient associée\n",
    "    #     au paramètre b (constante).\n",
    "    db = 0.0\n",
    "    for i in range(n):\n",
    "        db = db - (2/n) * (y[i] - a*x[i] - b)\n",
    "\n",
    "    return da, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3279951",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "gradient(x, y, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e3884",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14fa700df22ed5fe6245eb6fe67e22f9",
     "grade": false,
     "grade_id": "cell-d4a0f07dc684815b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pour afficher le caractère η dans Jupyter : \\eta<TABULATION>\n",
    "\n",
    "def optimisation(x, y, η=0.01, tol=1e-8, maxiter=1000, verbose=True):\n",
    "    \"\"\"\n",
    "    Boucle d'optimisation pour estimer les paramètres a et b de notre modèle\n",
    "    à partir des données.\n",
    "\n",
    "    INPUT\n",
    "    -----\n",
    "    \n",
    "    x : Numpy array, shape (n,)\n",
    "        Abscise des points de données.\n",
    "        \n",
    "    y : Numpy array, shape (n,)\n",
    "        Ordonnée des points de données.\n",
    "        \n",
    "    η : flat (optionel)\n",
    "        Pas d'optimisation.\n",
    "        \n",
    "    tol : flat (optionel)\n",
    "          Tolérance pour arrêter le calcul.\n",
    "          \n",
    "    maxiter : int (optionel)\n",
    "              Nombre maximum d'itérations possible avant d'arrêter le calcul.\n",
    "              \n",
    "    verbose : boolean (optionel)\n",
    "              Contrôle l'affichage à l'écran des informations au cours du calcul.\n",
    "    \n",
    "    RETURN\n",
    "    ------\n",
    "    \n",
    "    a : float\n",
    "        Estimation du coefficient directeur de la droite.\n",
    "        \n",
    "    b : float\n",
    "        Estimation de la constante du modèle.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --> Estimation initiale des paramètres.\n",
    "    a, b = 0.0, 0.0\n",
    "    \n",
    "    # --> Descente de gradient.\n",
    "    for i in range(maxiter):\n",
    "        # --> Calcul de la fonction objective.\n",
    "        erreur = objectif(x, y, a, b)\n",
    "        \n",
    "        # --> Calcul du gradient.\n",
    "        da, db = gradient(x, y, a, b)\n",
    "        \n",
    "        # --> Norme du gradient.\n",
    "        residu = da**2 + db**2\n",
    "        \n",
    "        # --> Affichage à l'écran.\n",
    "        if verbose:\n",
    "            print(\"Itération {0} :\".format(i))\n",
    "            print(\"     - Fonction objective : {0}\".format(erreur))\n",
    "            print(\"     - Norme du gradient : {0}\".format(residu))\n",
    "            print()\n",
    "        \n",
    "        # --> Test si la norme du gradient est suffisament petite.\n",
    "        if residu < tol:\n",
    "            break\n",
    "            \n",
    "        # --> Mise à jour des paramètres.\n",
    "        a = a - η*da\n",
    "        b = b - η*db\n",
    "    \n",
    "    # --> Affiche les coefficients obtenus à la fin.\n",
    "    if verbose:\n",
    "        print(\"\\n-------------------------------------\\n\\n\")\n",
    "        print(\"Estimation du coefficient directeur : {0}\".format(a))\n",
    "        print(\"Estimation de la constante : {0}\".format(b))\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ae669",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = optimisation(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59798a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36d98357da541f1871e1b7d2fff237bb",
     "grade": false,
     "grade_id": "cell-887c8701fa2d9f51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.2, label=\"Données\")\n",
    "\n",
    "z = np.linspace(-10, 10)\n",
    "ax.plot(z, a*z + b, color=\"red\", label=\"Modèle y = {0:0.3f}x + {1:0.3f}\".format(a, b))\n",
    "\n",
    "\n",
    "ax.set(xlim=(-10, 10), ylim=(-10, 10))\n",
    "ax.set(xlabel=\"x\", ylabel=\"y\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97577c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a5b183b67ffeae2c65cd4126d5b0f7c",
     "grade": false,
     "grade_id": "cell-3fab798c938bbeff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "optimisation(x, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7935c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TP\n",
    "\n",
    "C'est maintenant à vous de jouer.\n",
    "En utilisant les différentes fonctions **NumPy** vues notamment au cours du TP n°2, votre objectif est d'augmenter la rapidité d'exécution de chacune de ces fonctions.\n",
    "L'essentiel de votre travail devra se concentrer sur les fonctions `objectif` et `gradient` car c'est là que votre calcul passera la majeure partie de son temps.\n",
    "Pensez à ajouter en commentaire à chaque fois pourquoi vous pensez que les modifications que vous avez effectuées améliorent les performances du code.\n",
    "Pensez également à vérifier que ces mêmes modifications ne changent pas pour autant le résultat (i.e. que ce vous codez est effectivement correcte!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed5a92",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99ccf3e7a38f0aa89c649d0b378e3d86",
     "grade": true,
     "grade_id": "cell-1c3de87bc895cc4b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
